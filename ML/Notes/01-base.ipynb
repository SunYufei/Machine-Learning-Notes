{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 机器学习简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 如何分类\n",
    "\n",
    "按任务类型\n",
    "\n",
    "- 回归模型：预测某个无法枚举的数值\n",
    "- 分类模型：将样本分为两类或多类\n",
    "- 结构化学习模型：输出不是向量而是其他结构，如给定长文本聚集成短的总结\n",
    "\n",
    "按学习理论划分\n",
    "\n",
    "- 有监督学习：训练样本带有标签\n",
    "- 半监督学习：训练样本部分有标签；与使用所有标签数据相比，使用训练集的训练模型在训练时可以更为准确，而且训练成本更低\n",
    "- 无监督学习：训练样本全都没有标签，如聚类。根据样本间的相似性对样本集进行聚类试图使类内差距最小化，类间差距最大化\n",
    "- 强化学习：智能体（agent）通过与环境进行交互获得的奖赏来指导自己的行为，最终目标是使智能体获得最大的奖赏\n",
    "- 迁移学习：运用已存有的知识或者数据对不同担忧关联的领域问题进行求解；目的是通过迁移已有知识或者数据来解决目标领域中有标签样本数据比较少甚至没有的学习问题\n",
    "\n",
    "判别式模型和生成式模型\n",
    "- 判别方法：由数据直接学习决策函数 $Y=f(X)$，或者由条件分布概率 $P(Y|X)$ 作为预测模型为判别模型\n",
    "    - 常见判别模型：线性回归、boosting、SVM、决策树、感知机、线性判别分析 LDA、逻辑斯蒂回归等\n",
    "- 生成方法：由数据学习 $x$ 和 $y$ 的联合概率密度分布函数 $P(Y,X)$，然后通过贝叶斯公式求出条件概率分布 $P(Y|X)$ 作为预测的模型为生成模型\n",
    "    - 常见生成模型：朴素贝叶斯、隐马尔科夫模型、高斯混合模型、文档主题生成模型 LDA 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 性能度量\n",
    "\n",
    "衡量模型泛化能力的评价标准就是性能度量\n",
    "\n",
    "对比不同模型的能力时，使用不同的性能度量往往会导致不同的判别结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 回归问题常用的性能度量指标\n",
    "\n",
    "均方误差：$MSE=\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n}(f(x_i)-y_i)^2$\n",
    "\n",
    "均方根误差（RMSE），观测值与真值偏差的平方和与观测次数 $m$ 比值的平方根，用于衡量观测值同真值之间的偏差：$RMSE=\\sqrt{MSE}$\n",
    "\n",
    "和方误差：$SSE=\\displaystyle\\sum_{i=0}^{n}(f(x_i)-y_i)^2$\n",
    "\n",
    "平均绝对误差：$MAE=\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n}|f(x_i)-y_i|$\n",
    "\n",
    "平均绝对百分比误差：$MPAE=\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n}\\frac{|f(x_i)-y_i|}{y_i}$\n",
    "\n",
    "平均平方百分比误差：$MASE=\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n}\\left(\\frac{|f(x_i)-y_i|}{y_i}\\right)^2$\n",
    "\n",
    "决定系数：$R^2=1-\\displaystyle\\frac{SSE}{SST}=1-\\frac{\\sum_{i=0}^{n}(f(x_i)-y_i)^2}{\\sum_{i=0}^{n}(y_i-\\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 分类问题常用的性能度量指标\n",
    "\n",
    "- TP，FN，FP，TN\n",
    "    - TP，true positive，真正例，正样本被预测为正样本\n",
    "    - FN，false negative，假负例，正样本被预测为负样本\n",
    "    - FP，false positive，假正例，负样本被预测为正样本\n",
    "    - TN，true negative，真负例，负样本被预测为负样本\n",
    "    - 精确率（查准率） $Precision=\\displaystyle\\frac{TP}{TP+FP}$\n",
    "    - 召回率（查全率） $Recall=\\displaystyle\\frac{TP}{TP+FN}$\n",
    "    - 真正例率，正例被判断为正例的概率 $TPR=\\displaystyle\\frac{TP}{TP+FN}$\n",
    "    - 假正例率，反例被判断为正例的概率 $FPR=\\displaystyle\\frac{FP}{TM+FP}$\n",
    "    - $F_1$ 是精准率与召回率的调和平均值 $\\displaystyle\\frac{1}{F_1}=\\frac{1}{2}\\times\\frac{1}{Precision}+\\frac{1}{2}\\times\\frac{1}{Recall}$\n",
    "- 错误率与准确率\n",
    "    - 错误率 $e=\\displaystyle\\sum_{i=1}^{n}I(f(x_i\\neq y_i))$\n",
    "    - 准确率 $acc=1-e$\n",
    "- AUC 与 ROC 曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 特征工程\n",
    "\n",
    "数据和特征决定了及学习的上限，而模型和算法只是逼近了这个上限而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 无量纲化\n",
    "\n",
    "解决数据的量纲不同的问题，使不同的数据转换到同一规格\n",
    "\n",
    "（1）标准化\n",
    "\n",
    "假设前提时特征值服从正态分布，标准化后将其转换为标准正态分布\n",
    "\n",
    "在深度学习里，将数据标准化到一个特定的范围能够在反向传播中保证更好的收敛\n",
    "\n",
    "如果不进行数据标准化，有些特征（值很大）将会对损失函数影响更大，使得其他值比较小的特征重要性降低\n",
    "\n",
    "数据标准化可以使得每个特征的重要性更加均衡\n",
    "\n",
    "$$\n",
    "x'=\\displaystyle\\frac{x-\\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x1 = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "（2）归一化/区间缩放法\n",
    "\n",
    "利用边界值信息，将特征的取值区间缩放到某个特点的范围，如 $[0,1]$ 等\n",
    "\n",
    "适用于数据量较小的工程，利用两个最值进行缩放\n",
    "\n",
    "$$\n",
    "x'=\\displaystyle\\frac{x-\\min}{\\max - \\min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x1 = MinMaxScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1.2 哑编码与独热编码\n",
    "\n",
    "将离散特征的取值（如中国、美国、德国）扩展到了欧氏空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 独热编码\n",
    "OneHotEncoder().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1.3 缺失值补充\n",
    "\n",
    "常用方法：均值、就近补齐、$K-$最近距离填充等\n",
    "\n",
    "有时候缺失值也是一种特征，可以补充一列将数据缺失与否赋值为0、1\n",
    "\n",
    "缺失值太多，可以直接舍弃该列特征，否则可能会带来较大噪声\n",
    "\n",
    "如果缺失值较少时（如少于$10\\%$），可以考虑对缺失值进行填充\n",
    "\n",
    "填充策略：\n",
    "\n",
    "1. 用一个异常值填充（如0或-999）并将缺失值作为一个特征处理\n",
    "2. 用均值或者条件均值填充\n",
    "    1. 数据是不平衡的，那么应该使用条件均值填充\n",
    "    2. 条件均值指与缺失值所属标签相同的所有数据的均值\n",
    "3. 用相邻数据填充\n",
    "4. 利用插值算法\n",
    "5. 数据拟合\n",
    "    1. 将缺失值作为一个预测问题来处理\n",
    "    2. 将数据分为正常数据和缺失数据，对有值的数据采用随机森林等方法拟合，然后对有缺失值的数据用预测的值来填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 特征选择\n",
    "\n",
    "选入大量的特征不仅会降低模型效果，也会耗费大量的计算时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 方差选择法\n",
    "\n",
    "若某列特征数值变化一直平缓，说明这个特征对结果的影响较小\n",
    "\n",
    "可以计算各个特征的方差，选择方差大于自设阈值的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "VarianceThreshold(threshold=10).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2.2 相关系数、统计检验\n",
    "\n",
    "pearson 相关系数，应用于连续变量\n",
    "\n",
    "卡方检验，应用于离散变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# person 相关系数\n",
    "from scipy.stats import pearsonr\n",
    "SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=5).fit_transform(x, y)\n",
    "\n",
    "# 卡方检验\n",
    "from sklearn.feature_selection import chi2\n",
    "SelectKBest(chi2, k=5).fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2.3 互信息法\n",
    "\n",
    "评价自变量对因变量的相关性\n",
    "\n",
    "$$\n",
    "I(X;Y)=\\displaystyle\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 基于机器学习的特征选择法\n",
    "\n",
    "针对特征和响应变量建立预测模型，例如用基于树的方法（决策树、随机森林、GBDT），或者扩展的线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "SelectFromModel(GradientBoostingClassifier()).fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 降维特征\n",
    "\n",
    "若特征矩阵过大，会导致训练时间过长，需要降低特征矩阵维度\n",
    "\n",
    "降维是通过保留重要的特征，减少数据特征的维度\n",
    "\n",
    "特征的重要性取决于该特征能够表达多少数据集的信息，也取决于使用什么方法进行降维\n",
    "\n",
    "降维的好处由节省存储空间，加快计算速度，避免模型过拟合等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 主成分分析法 PCA\n",
    "\n",
    "将数据变换到一个新的坐标系统中的线性变换\n",
    "\n",
    "主要目的是为让映射后得到的向量具有最大的不相关性\n",
    "\n",
    "PCA 追求的是在降维之后能够最大化保持数据的内在信息，并通过衡量在投影方向上的数据方差的大小来衡量该方向的重要性\n",
    "\n",
    "（1）计算相关系数矩阵\n",
    "\n",
    "$$\n",
    "R=\n",
    "\\begin{bmatrix}\n",
    "r_{11} & ... & r_{1p} \\\\\n",
    ". & & . \\\\\n",
    "r_{p1} & ... & r_{pp} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$r_{ij}(i,j=1,2,...,p)$为原变量$x_i$与$y_j$的相关系数，其计算公式为\n",
    "\n",
    "$$\n",
    "r_{ij}=\\displaystyle\\frac{\\sum_{k=1}^n(x_{ki}-\\bar{x_i})(x_{kj}-\\bar{x_j})}{(x_{ki}-\\bar{x_i})^2(x_{kj}-\\bar{x_j})^2}\n",
    "$$\n",
    "\n",
    "（2）计算特征值与特征向量\n",
    "\n",
    "解特征方程 $|\\lambda I-R|=0$，用雅可比法求出特征值，并使其按大小顺序排列 $\\lambda_1\\geq\\lambda_2\\geq ... \\geq\\lambda_p\\geq 0$\n",
    "\n",
    "特征值 $\\lambda_i$ 对应的特征向量为 $e_i$，且 $||e||=1$\n",
    "\n",
    "（3）计算主成分贡献率及累计贡献率\n",
    "\n",
    "对应的单位特征向量 $e_i$ 就是主成分 $z_i$ 的关于原变量的系数，即 $z_i=xe_{i}^{T}$\n",
    "\n",
    "贡献率：$\\alpha_i=\\displaystyle\\frac{\\lambda_i}{\\sum_{k=1}^p\\lambda_k},i=1,2,...,p$\n",
    "\n",
    "累计贡献率：$\\displaystyle\\frac{\\sum_{k=1}^i\\lambda_k}{\\sum_{k=1}^p\\lambda_k},i=1,2,...,p$\n",
    "\n",
    "一般取累计贡献率达 $85\\%\\sim 95\\%$ 的特征值 $\\lambda_1,\\lambda_2,...,\\lambda_m$ 所对应的第1、2、...、第 $m(m\\leq p)$ 个主成分 $z_1,z_2,...,z_m$\n",
    "\n",
    "（4）计算主成分载荷\n",
    "\n",
    "主成分载荷是反映主成分 $z_i$ 与原变量 $x_j$ 之间的相互关联程度\n",
    "\n",
    "$$\n",
    "l_{ij}=p(z_i,x_j)=\\sqrt{\\lambda_i}e_{ij}(i,j=1,2,...,p)\n",
    "$$\n",
    "\n",
    "将变量标准化后再计算其协方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# k 为主成分的数目\n",
    "PCA(n_components=k).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.3.2 线性判别分析法 LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 局部线性嵌入 LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 过拟合、欠拟合与正则化\n",
    "\n",
    "过拟合的表现是训练数据误差逐渐减小，验证集上误差增大\n",
    "\n",
    "训练初期由于训练不足，学习器的拟合能力不够强，偏差比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 过拟合与欠拟合的区别，什么是正则化\n",
    "\n",
    "欠拟合，模型不能在训练集上获得足够低的训练误差，由于特征维度过少，导致拟合的函数无法满足训练集，导致误差较大\n",
    "\n",
    "过拟合，模型的训练误差与测试误差（泛化误差）之间的差距过大\n",
    "\n",
    "正则化，减少测试误差的策略，代价可能增大训练误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 解决欠拟合的方法\n",
    "\n",
    "1. 加入新的特征\n",
    "    1. 深度学习，利用因子分解机、子编码器等\n",
    "2. 增加模型复杂度\n",
    "    1. 线性模型增加高次项\n",
    "    2. 深度学习增加网络层数、神经元个数\n",
    "3. 减少正则化项的系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 防止过拟合的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 正则化\n",
    "\n",
    "$L_1$ 正则化，减少参数的绝对值总和，$||x||_1=\\sum_{i}|x_i|$\n",
    "\n",
    "$L_2$ 正则化，减少参数平方的总和，$||x||_2=\\sum_{i}x_i^2$\n",
    "\n",
    "混合 $L_1$ 与 $L_2$ 正则化，$\\alpha||x||_1+\\displaystyle\\frac{1-\\alpha}{2}||x||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Batch Normalization\n",
    "\n",
    "作用是缓解梯度消失，加速网络的训练，防止过拟合，降低了参数初始化的要求\n",
    "\n",
    "由于训练数据和测试数据的分布不同回降低模型的泛化能力，因此应该在开始训练前对所有数据做归一化处理\n",
    "\n",
    "Batch Normalization 会针对每一批数据在网络的每一层输入之前增加归一化处理，目的是为了使输入均值为 0，标准差为 1\n",
    "\n",
    "利用公式表示就是针对每层的第 $k$ 个神经元，计算这一批数据在第 $k$ 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值\n",
    "\n",
    "$$\n",
    "\\hat{x_k}=\\displaystyle\\frac{x_k-E[x_k]}{\\sqrt{var[x_k+\\epsilon]}}\n",
    "$$\n",
    "\n",
    "Batch Normalization 通过对数据分布进行额外约束来增强模型的泛化能力，同时由于破坏了之前学到的特征分布，从而也降低了模型的拟合能力\n",
    "\n",
    "为了恢复数据的原始分布，Batch Normalization 引入了一个重构变换来还原最优的输入数据分布\n",
    "\n",
    "$$\n",
    "y_k=BK(x_k)=\\gamma\\hat{x_k}+\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Dropout\n",
    "\n",
    "对神经网络进行随即删减，降低网络复杂度\n",
    "\n",
    "具体过程\n",
    "\n",
    "1. 第一次迭代时随机删除一部分的隐藏单元，保持输入输出层不变，更新神经网络中的权值\n",
    "2. 第二次迭代时随机删除掉一部分，和上一次删除的不一样\n",
    "3. 第三次以及之后都是这样，直至训练结束\n",
    "\n",
    "运用 Dropout 相当于训练了非常多个仅有部分隐层单元的神经网络，每个这种神经网络都能够给出一个分类结果，这些结果有的是正确的，有的是错误的\n",
    "\n",
    "随着训练的进行，大部分网络都能够给出正确的分类结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 迭代截断\n",
    "\n",
    "模型对训练数据集迭代收敛之前停止迭代防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.5 交叉验证\n",
    "\n",
    "$k-$fold 交叉验证是把训练样例分成 $k$ 份，然后进行 $k$ 次交叉验证过程，每次使用不同的一份样本作为验证集合，其余 $k-1$ 份样本合并作为训练集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 偏差与方差\n",
    "\n",
    "偏差度量了学习算法的期望预测与真是结果的偏离程度\n",
    "\n",
    "偏差用于描述模型的拟合能力，方差用于描述模型的稳定性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 泛化误差、偏差、方差与噪声之间的关系\n",
    "\n",
    "$D$：训练集\n",
    "\n",
    "$y$：$x$ 真实的标记\n",
    "\n",
    "$y_D$：$x$ 在训练集 $D$ 中的标记\n",
    "\n",
    "$F$：通过训练集 $D$ 学到的模型\n",
    "\n",
    "$f(x;D)$：由训练集 $D$ 学到的模型 $f$ 对 $x$ 的预测输出\n",
    "\n",
    "$\\bar{f}(x)$：模型 $f$ 对 $x$ 的期望预测输出\n",
    "\n",
    "学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差被称为“泛化误差”\n",
    "\n",
    "对于泛化误差，以回归任务为例，期望泛化误差为：$Err(x)=E[(y_D-f(x;D))^2]$\n",
    "\n",
    "方差公式：$var(x)=E[(f(x;D)-\\bar{f}(x))^2]$\n",
    "\n",
    "噪声即为真实标记与数据集中的实际标记间的偏差，公式为：$\\epsilon=E[(y_D-y)^2]$\n",
    "\n",
    "假定噪声期望为 0，即 $E(y_D-y)=0$\n",
    "\n",
    "偏差即为期望预测与真实标记的误差，偏差平方的公式为：$bias^2(x)=(\\bar{f}(x)-y)^2$\n",
    "\n",
    "则 $Err(x)=bias^2(x)+\\epsilon+var(x)$，泛化误差可分解为偏差、方差和噪声之和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 导致偏差和方差的原因\n",
    "\n",
    "偏差是由于模型的复杂度不够或者对学习算法做了错误的假设\n",
    "\n",
    "训练误差主要由偏差造成的\n",
    "\n",
    "方差通常是由于模型的复杂度过高导致的\n",
    "\n",
    "由方差引起的误差通常体现在测试误差相对训练误差的变化上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
